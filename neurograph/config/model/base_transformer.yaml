name: 'Transformer'
data_type: 'dense'
n_classes: 2

# num transformer layer
num_layers: 1
hidden_dim: 8

# self-attn params (the same for each layer)
num_heads: 2
attn_dropout: 0.2
return_attn: False

# transformer block MLP parameters
# 2-layer MLP
mlp_dropout: 0.2
mlp_hidden_multiplier: 2.0
mlp_act_func: 'GELU'
mlp_act_func_params: null

# final pooling
pooling: 'concat'

# final MLP head config
head_config:
    act_func: null
    act_func_params: null
    layers:
        - {out_size: 4, dropout: 0.5, act_func: 'GELU', act_func_params: null}
